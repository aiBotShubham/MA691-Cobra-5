{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "JvHXhyzea-mx"
   },
   "outputs": [],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ZbnqQmUVbhtH"
   },
   "outputs": [],
   "source": [
    "# Dataset Functions\n",
    "##### USER-DEFINED FUNCTIONS\n",
    "def f_get_Normalization(X, norm_mode):    \n",
    "    num_Patient, num_Feature = np.shape(X)\n",
    "    \n",
    "    if norm_mode == 'standard': #zero mean unit variance\n",
    "        for j in range(num_Feature):\n",
    "            if np.nanstd(X[:,j]) != 0:\n",
    "                X[:,j] = (X[:,j] - np.nanmean(X[:, j]))/np.nanstd(X[:,j])\n",
    "            else:\n",
    "                X[:,j] = (X[:,j] - np.nanmean(X[:, j]))\n",
    "    elif norm_mode == 'normal': #min-max normalization\n",
    "        for j in range(num_Feature):\n",
    "            X[:,j] = (X[:,j] - np.nanmin(X[:,j]))/(np.nanmax(X[:,j]) - np.nanmin(X[:,j]))\n",
    "    else:\n",
    "        print(\"INPUT MODE ERROR!\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
    "    '''\n",
    "        mask3 is required to get the contional probability (to calculate the denominator part)\n",
    "        mask3 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(meas_time)[0], num_Event, num_Category]) # for denominator\n",
    "    for i in range(np.shape(meas_time)[0]):\n",
    "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
    "    '''\n",
    "        mask4 is required to get the log-likelihood loss \n",
    "        mask4 size is [N, num_Event, num_Category]\n",
    "            if not censored : one element = 1 (0 elsewhere)\n",
    "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
    "    for i in range(np.shape(time)[0]):\n",
    "        if label[i,0] != 0:  #not censored\n",
    "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
    "        else: #label[i,2]==0: censored\n",
    "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask3(time, meas_time, num_Category):\n",
    "    '''\n",
    "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
    "        mask5 size is [N, num_Category]. \n",
    "        - For longitudinal measurements:\n",
    "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
    "             denom is not needed since comparing is done over the same denom\n",
    "        - For single measurement:\n",
    "             1's from start to the event time(inclusive)\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
    "    if np.shape(meas_time):  #lonogitudinal measurements \n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t1 = int(meas_time[i, 0]) # last measurement time\n",
    "            t2 = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    else:                    #single measurement\n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "##### TRANSFORMING DATA\n",
    "def f_construct_dataset(df, feat_list):\n",
    "    '''\n",
    "        id   : patient indicator\n",
    "        tte  : time-to-event or time-to-censoring\n",
    "            - must be synchronized based on the reference time\n",
    "        times: time at which observations are measured\n",
    "            - must be synchronized based on the reference time (i.e., times start from 0)\n",
    "        label: event/censoring information\n",
    "            - 0: censoring\n",
    "            - 1: event type 1\n",
    "            - 2: event type 2\n",
    "            ...\n",
    "    '''\n",
    "\n",
    "    grouped  = df.groupby(['id'])\n",
    "    id_list  = pd.unique(df['id'])\n",
    "    max_meas = np.max(grouped.count())[0]\n",
    "\n",
    "    data     = np.zeros([len(id_list), max_meas, len(feat_list)+1])\n",
    "    pat_info = np.zeros([len(id_list), 5])\n",
    "\n",
    "    for i, tmp_id in enumerate(id_list):\n",
    "        tmp = grouped.get_group(tmp_id).reset_index(drop=True)\n",
    "\n",
    "        pat_info[i,4] = tmp.shape[0]                                   #number of measurement\n",
    "        pat_info[i,3] = np.max(tmp['times'])     #last measurement time\n",
    "        pat_info[i,2] = tmp['label'][0]      #cause\n",
    "        pat_info[i,1] = tmp['tte'][0]         #time_to_event\n",
    "        pat_info[i,0] = tmp['id'][0]      \n",
    "\n",
    "        data[i, :int(pat_info[i, 4]), 1:]  = tmp[feat_list]\n",
    "        data[i, :int(pat_info[i, 4]-1), 0] = np.diff(tmp['times'])\n",
    "    \n",
    "    return pat_info, data\n",
    "\n",
    "\n",
    "def import_dataset(norm_mode = 'standard'):\n",
    "\n",
    "    df_                = pd.read_csv('pbc2_cleaned.csv')\n",
    "\n",
    "    bin_list           = ['drug', 'sex', 'ascites', 'hepatomegaly', 'spiders']\n",
    "    cont_list          = ['age', 'edema', 'serBilir', 'serChol', 'albumin', 'alkaline', 'SGOT', 'platelets', 'prothrombin', 'histologic']\n",
    "    feat_list          = cont_list + bin_list\n",
    "    df_                = df_[['id', 'tte', 'times', 'label']+feat_list]\n",
    "    df_org_            = df_.copy(deep=True)\n",
    "\n",
    "    df_[cont_list]     = f_get_Normalization(np.asarray(df_[cont_list]).astype(float), norm_mode)\n",
    "\n",
    "    pat_info, data     = f_construct_dataset(df_, feat_list)\n",
    "    _, data_org        = f_construct_dataset(df_org_, feat_list)\n",
    "\n",
    "    data_mi                  = np.zeros(np.shape(data))\n",
    "    data_mi[np.isnan(data)]  = 1\n",
    "    data_org[np.isnan(data)] = 0\n",
    "    data[np.isnan(data)]     = 0 \n",
    "\n",
    "    x_dim           = np.shape(data)[2] # 1 + x_dim_cont + x_dim_bin (including delta)\n",
    "    x_dim_cont      = len(cont_list)\n",
    "    x_dim_bin       = len(bin_list) \n",
    "\n",
    "    last_meas       = pat_info[:,[3]]  #pat_info[:, 3] contains age at the last measurement\n",
    "    label           = pat_info[:,[2]]  #two competing risks\n",
    "    time            = pat_info[:,[1]]  #age when event occurred\n",
    "\n",
    "    num_Category    = int(np.max(pat_info[:, 1]) * 1.2) #or specifically define larger than the max tte\n",
    "    # num_Event       = len(np.unique(label)) - 1\n",
    "    num_Event       = 1\n",
    "    if num_Event == 1:\n",
    "        label[np.where(label!=0)] = 1 #make single risk\n",
    "\n",
    "    mask1           = f_get_fc_mask1(last_meas, num_Event, num_Category)\n",
    "    mask2           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
    "    mask3           = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "    DIM             = (x_dim, x_dim_cont, x_dim_bin)\n",
    "    DATA            = (data, time, label)\n",
    "    # DATA            = (data, data_org, time, label)\n",
    "    MASK            = (mask1, mask2, mask3)\n",
    "\n",
    "    return DIM, DATA, MASK, data_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "0XK4ZMw5m7Rk"
   },
   "outputs": [],
   "source": [
    "# ! pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "umDVWr7apNq4"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JrEPncdlhuTU"
   },
   "outputs": [],
   "source": [
    "# Longitudional Model Functions\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x + _EPSILON)\n",
    "\n",
    "def div(x, y):\n",
    "    return tf.div(x, (y + _EPSILON))\n",
    "\n",
    "def get_seq_length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    tmp_length = tf.reduce_sum(used, 1)\n",
    "    tmp_length = tf.cast(tmp_length, tf.int32)\n",
    "    return tmp_length\n",
    "\n",
    "\n",
    "class Model_Longitudinal_Attention:\n",
    "    def __init__(self, sess, name, input_dims, network_settings):\n",
    "        self.sess               = sess\n",
    "        self.name               = name\n",
    "\n",
    "        # INPUT DIMENSIONS\n",
    "        self.x_dim              = input_dims['x_dim']\n",
    "        self.x_dim_cont         = input_dims['x_dim_cont']\n",
    "        self.x_dim_bin          = input_dims['x_dim_bin']\n",
    "\n",
    "        self.num_Event          = input_dims['num_Event']\n",
    "        self.num_Category       = input_dims['num_Category']\n",
    "        self.max_length         = input_dims['max_length']\n",
    "\n",
    "        # NETWORK HYPER-PARMETERS\n",
    "        self.h_dim1             = network_settings['h_dim_RNN']\n",
    "        self.h_dim2             = network_settings['h_dim_FC']\n",
    "        self.num_layers_RNN     = network_settings['num_layers_RNN']\n",
    "        self.num_layers_ATT     = network_settings['num_layers_ATT']\n",
    "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
    "\n",
    "        self.RNN_type           = network_settings['RNN_type']\n",
    "\n",
    "        self.FC_active_fn       = network_settings['FC_active_fn']\n",
    "        self.RNN_active_fn      = network_settings['RNN_active_fn']\n",
    "        self.initial_W          = network_settings['initial_W']\n",
    "        \n",
    "        self.reg_W              = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W'])\n",
    "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W_out'])\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            #### PLACEHOLDER DECLARATION\n",
    "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "\n",
    "            self.lr_rate     = tf.placeholder(tf.float32)\n",
    "            self.keep_prob   = tf.placeholder(tf.float32)                                                      #keeping rate\n",
    "            self.a           = tf.placeholder(tf.float32)\n",
    "            self.b           = tf.placeholder(tf.float32)\n",
    "            self.c           = tf.placeholder(tf.float32)\n",
    "\n",
    "            self.x           = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])\n",
    "            self.x_mi        = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])           #this is the missing indicator (including for cont. & binary) (includes delta)\n",
    "            self.k           = tf.placeholder(tf.float32, shape=[None, 1])                                     #event/censoring label (censoring:0)\n",
    "            self.t           = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for denominator\n",
    "            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for Loss 1\n",
    "            self.fc_mask3    = tf.placeholder(tf.float32, shape=[None, self.num_Category])                     #for Loss 2\n",
    "\n",
    "            \n",
    "            seq_length     = get_seq_length(self.x)\n",
    "            tmp_range      = tf.expand_dims(tf.range(0, self.max_length, 1), axis=0)\n",
    "            \n",
    "            self.rnn_mask1 = tf.cast(tf.less_equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32)            \n",
    "            self.rnn_mask2 = tf.cast(tf.equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32) \n",
    "            \n",
    "            \n",
    "            ### DEFINE LOOP FUNCTION FOR RAW_RNN w/ TEMPORAL ATTENTION\n",
    "            def loop_fn_att(time, cell_output, cell_state, loop_state):\n",
    "\n",
    "                emit_output = cell_output \n",
    "\n",
    "                if cell_output is None:  # time == 0\n",
    "                    next_cell_state = cell.zero_state(self.mb_size, tf.float32)\n",
    "                    next_loop_state = loop_state_ta\n",
    "                else:\n",
    "                    next_cell_state = cell_state\n",
    "                    tmp_h = create_concat_state(next_cell_state, self.num_layers_RNN, self.RNN_type)\n",
    "\n",
    "                    e = create_FCNet(tf.concat([tmp_h, all_last], axis=1), self.num_layers_ATT, self.h_dim2, \n",
    "                                           tf.nn.tanh, 1, None, self.initial_W, keep_prob=self.keep_prob)\n",
    "                    e = tf.exp(e)\n",
    "\n",
    "                    next_loop_state = (loop_state[0].write(time-1, e),                # save att power (e_{j})\n",
    "                                       loop_state[1].write(time-1, tmp_h))  # save all the hidden states\n",
    "\n",
    "                # elements_finished = (time >= seq_length)\n",
    "                elements_finished = (time >= self.max_length-1)\n",
    "\n",
    "                #this gives the break-point (no more recurrence after the max_length)\n",
    "                finished = tf.reduce_all(elements_finished)    \n",
    "                next_input = tf.cond(finished, lambda: tf.zeros([self.mb_size, 2*self.x_dim], dtype=tf.float32),  # [x_hist, mi_hist]\n",
    "                                               lambda: inputs_ta.read(time))\n",
    "\n",
    "                return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "\n",
    "            \n",
    "            # divide into the last x and previous x's\n",
    "            x_last = tf.slice(self.x, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
    "            x_last = tf.reshape(x_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
    "\n",
    "            x_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
    "            x_last = tf.slice(x_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
    "            x_hist = self.x * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
    "            x_hist = tf.slice(x_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
    "\n",
    "            # do same thing for missing indicator\n",
    "            mi_last = tf.slice(self.x_mi, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
    "            mi_last = tf.reshape(mi_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
    "\n",
    "            mi_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x_mi, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
    "            mi_last = tf.slice(mi_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
    "            mi_hist = self.x_mi * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
    "            mi_hist = tf.slice(mi_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
    "\n",
    "            all_hist = tf.concat([x_hist, mi_hist], axis=2)\n",
    "            all_last = tf.concat([x_last, mi_last], axis=1)\n",
    "\n",
    "\n",
    "            #extract inputs for the temporal attention: mask (to incorporate only the measured time) and x_{M}\n",
    "            seq_length     = get_seq_length(x_hist)\n",
    "            rnn_mask_att   = tf.cast(tf.not_equal(tf.reduce_sum(x_hist, reduction_indices=2), 0), dtype=tf.float32)  #[mb_size, max_length-1], 1:measurements 0:no measurements\n",
    "            \n",
    "\n",
    "            ##### SHARED SUBNETWORK: RNN w/ TEMPORAL ATTENTION\n",
    "            #change the input tensor to TensorArray format with [max_length, mb_size, x_dim]\n",
    "            inputs_ta = tf.TensorArray(dtype=tf.float32, size=self.max_length-1).unstack(_transpose_batch_time(all_hist), name = 'Shared_Input')\n",
    "\n",
    "\n",
    "            #create a cell with RNN hyper-parameters (RNN types, #layers, #nodes, activation functions, keep proability)\n",
    "            cell = create_rnn_cell(self.h_dim1, self.num_layers_RNN, self.keep_prob, \n",
    "                                         self.RNN_type, self.RNN_active_fn)\n",
    "\n",
    "            #define the loop_state TensorArray for information from rnn time steps\n",
    "            loop_state_ta = (tf.TensorArray(size=self.max_length-1, dtype=tf.float32),  #e values (e_{j})\n",
    "                             tf.TensorArray(size=self.max_length-1, dtype=tf.float32))  #hidden states (h_{j})\n",
    "            \n",
    "            rnn_outputs_ta, self.rnn_final_state, loop_state_ta = tf.nn.raw_rnn(cell, loop_fn_att)\n",
    "            #rnn_outputs_ta  : TensorArray\n",
    "            #rnn_final_state : Tensor\n",
    "            #rnn_states_ta   : (TensorArray, TensorArray)\n",
    "\n",
    "            rnn_outputs = _transpose_batch_time(rnn_outputs_ta.stack())\n",
    "            # rnn_outputs =  tf.reshape(rnn_outputs, [-1, self.max_length-1, self.h_dim1])\n",
    "\n",
    "            rnn_states  = _transpose_batch_time(loop_state_ta[1].stack())\n",
    "\n",
    "            att_weight  = _transpose_batch_time(loop_state_ta[0].stack()) #e_{j}\n",
    "            att_weight  = tf.reshape(att_weight, [-1, self.max_length-1]) * rnn_mask_att # masking to set 0 for the unmeasured e_{j}\n",
    "\n",
    "            #get a_{j} = e_{j}/sum_{l=1}^{M-1}e_{l}\n",
    "            self.att_weight  = div(att_weight,(tf.reduce_sum(att_weight, axis=1, keepdims=True) + _EPSILON)) #softmax (tf.exp is done, previously)\n",
    "\n",
    "            # 1) expand att_weight to hidden state dimension, 2) c = \\sum_{j=1}^{M} a_{j} x h_{j}\n",
    "            self.context_vec = tf.reduce_sum(tf.tile(tf.reshape(self.att_weight, [-1, self.max_length-1, 1]), [1, 1, self.num_layers_RNN*self.h_dim1]) * rnn_states, axis=1)\n",
    "\n",
    "\n",
    "            self.z_mean      = FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_mean1\")\n",
    "            self.z_std       = tf.exp(FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_std1\"))\n",
    "\n",
    "            epsilon          = tf.random_normal([self.mb_size, self.max_length-1, self.x_dim], mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "            self.z           = self.z_mean + self.z_std * epsilon\n",
    "\n",
    "            \n",
    "            ##### CS-SPECIFIC SUBNETWORK w/ FCNETS \n",
    "            inputs = tf.concat([x_last, self.context_vec], axis=1)\n",
    "\n",
    "\n",
    "            #1 layer for combining inputs\n",
    "            h = FC_Net(inputs, self.h_dim2, activation_fn=self.FC_active_fn, weights_initializer=self.initial_W, scope=\"Layer1\")\n",
    "            h = tf.nn.dropout(h, keep_prob=self.keep_prob)\n",
    "\n",
    "            # (num_layers_CS-1) layers for cause-specific (num_Event subNets)\n",
    "            out = []\n",
    "            for _ in range(self.num_Event):\n",
    "                cs_out = create_FCNet(h, (self.num_layers_CS), self.h_dim2, self.FC_active_fn, self.h_dim2, self.FC_active_fn, self.initial_W, self.reg_W, self.keep_prob)\n",
    "                out.append(cs_out)\n",
    "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
    "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim2])\n",
    "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
    "\n",
    "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
    "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
    "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
    "\n",
    "\n",
    "            ##### GET LOSS FUNCTIONS\n",
    "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
    "            self.loss_Ranking()             #get loss2: Ranking loss\n",
    "            self.loss_RNN_Prediction()      #get loss3: RNN prediction loss\n",
    "\n",
    "            self.LOSS_TOTAL     = self.a*self.LOSS_1 + self.b*self.LOSS_2 + self.c*self.LOSS_3 + tf.losses.get_regularization_loss()\n",
    "            self.LOSS_BURNIN    = self.LOSS_3 + tf.losses.get_regularization_loss()\n",
    "\n",
    "            self.solver         = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
    "            self.solver_burn_in = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_BURNIN)\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
    "    def loss_Log_Likelihood(self):\n",
    "        sigma3 = tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "        I_1 = tf.sign(self.k)\n",
    "        denom = 1 - tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True) # make subject specific denom.\n",
    "        denom = tf.clip_by_value(denom, tf.cast(_EPSILON, dtype=tf.float32), tf.cast(1.-_EPSILON, dtype=tf.float32))\n",
    "\n",
    "        #for uncenosred: log P(T=t,K=k|x,Y,t>t_M)\n",
    "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
    "        tmp1 = I_1 * log(div(tmp1,denom))\n",
    "\n",
    "        #for censored: log \\sum P(T>t|x,Y,t>t_M)\n",
    "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
    "        tmp2 = (1. - I_1) * log(div(tmp2,denom))\n",
    "\n",
    "        self.LOSS_1 = - tf.reduce_mean(tmp1 + sigma3*tmp2)\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
    "    def loss_Ranking(self):\n",
    "        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
    "\n",
    "        eta = []\n",
    "        for e in range(self.num_Event):\n",
    "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
    "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
    "            I_2 = tf.diag(tf.squeeze(I_2))\n",
    "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
    "\n",
    "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask3)) #no need to divide by each individual dominator\n",
    "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
    "\n",
    "            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n",
    "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
    "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
    "\n",
    "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
    "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
    "\n",
    "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
    "\n",
    "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keepdims=True)\n",
    "\n",
    "            eta.append(tmp_eta)\n",
    "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
    "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keepdims=True)\n",
    "\n",
    "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 3 -- RNN prediction loss\n",
    "    def loss_RNN_Prediction(self):\n",
    "        tmp_x  = tf.slice(self.x, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
    "        tmp_mi = tf.slice(self.x_mi, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
    "\n",
    "        tmp_mask1  = tf.tile(tf.expand_dims(self.rnn_mask1, axis=2), [1,1,self.x_dim]) #for hisotry (1...J-1)\n",
    "        tmp_mask1  = tmp_mask1[:, :(self.max_length-1), :] \n",
    "\n",
    "        zeta = tf.reduce_mean(tf.reduce_sum(tmp_mask1 * (1. - tmp_mi) * tf.pow(self.z - tmp_x, 2), reduction_indices=1))  #loss calculated for selected features.\n",
    "\n",
    "        self.LOSS_3 = zeta\n",
    "\n",
    " \n",
    "    def get_cost(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb)        = DATA\n",
    "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
    "        (x_mi_mb)                 = MISSING\n",
    "        (alpha, beta, gamma)      = PARAMETERS\n",
    "        return self.sess.run(self.LOSS_TOTAL, \n",
    "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
    "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
    "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "\n",
    "    def train(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb)        = DATA\n",
    "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
    "        (x_mi_mb)                 = MISSING\n",
    "        (alpha, beta, gamma)      = PARAMETERS\n",
    "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
    "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb,\n",
    "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
    "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "    \n",
    "    def train_burn_in(self, DATA, MISSING, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb)        = DATA\n",
    "        (x_mi_mb)                 = MISSING\n",
    "\n",
    "        return self.sess.run([self.solver_burn_in, self.LOSS_3], \n",
    "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "    \n",
    "    def predict(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_z(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.z, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_rnnstate(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.rnn_final_state, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_att(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.att_weight, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_context_vec(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.context_vec, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def get_z_mean_and_std(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run([self.z_mean, self.z_std], feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "16KxNgrojjDa"
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "\n",
    "\n",
    "### CONSTRUCT MULTICELL FOR MULTI-LAYER RNNS\n",
    "def create_rnn_cell(num_units, num_layers, keep_prob, RNN_type, activation_fn): \n",
    "    '''\n",
    "        GOAL         : create multi-cell (including a single cell) to construct multi-layer RNN\n",
    "        num_units    : number of units in each layer\n",
    "        num_layers   : number of layers in MulticellRNN\n",
    "        keep_prob    : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
    "        RNN_type     : either 'LSTM' or 'GRU'\n",
    "    '''\n",
    "    if activation_fn == 'None':\n",
    "        activation_fn = tf.nn.tanh\n",
    "\n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        if RNN_type == 'GRU':\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units, activation=activation_fn)\n",
    "        elif RNN_type == 'LSTM':\n",
    "            cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn, state_is_tuple=True)\n",
    "            # cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn)\n",
    "        if not keep_prob is None:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob) # state_keep_prob=keep_prob\n",
    "        cells.append(cell)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    return cell\n",
    "\n",
    "\n",
    "### EXTRACT STATE OUTPUT OF MULTICELL-RNNS\n",
    "def create_concat_state(state, num_layers, RNN_type, BiRNN=None):\n",
    "    '''\n",
    "        GOAL\t     : concatenate the tuple-type tensor (state) into a single tensor\n",
    "        state        : input state is a tuple ofo MulticellRNN (i.e. output of MulticellRNN)\n",
    "                       consist of only hidden states h for GRU and hidden states c and h for LSTM\n",
    "        num_layers   : number of layers in MulticellRNN\n",
    "        RNN_type     : either 'LSTM' or 'GRU'\n",
    "    '''\n",
    "    for i in range(num_layers):\n",
    "        if BiRNN != None:\n",
    "            if RNN_type == 'LSTM':\n",
    "                tmp = tf.concat([state[0][i][1], state[1][i][1]], axis=1) ## i-th layer, h state for LSTM\n",
    "            elif RNN_type == 'GRU':\n",
    "                tmp = tf.concat([state[0][i], state[1][i]], axis=1) ## i-th layer, h state for GRU\n",
    "            else:\n",
    "                print('ERROR: WRONG RNN CELL TYPE')\n",
    "        else:\n",
    "            if RNN_type == 'LSTM':\n",
    "                tmp = state[i][1] ## i-th layer, h state for LSTM\n",
    "            elif RNN_type == 'GRU':\n",
    "                tmp = state[i] ## i-th layer, h state for GRU\n",
    "            else:\n",
    "                print('ERROR: WRONG RNN CELL TYPE')\n",
    "\n",
    "        if i == 0:\n",
    "            rnn_state_out = tmp\n",
    "        else:\n",
    "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
    "    \n",
    "    return rnn_state_out\n",
    "\n",
    "\n",
    "### FEEDFORWARD NETWORK\n",
    "def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, w_reg=None, keep_prob=1.0):\n",
    "    '''\n",
    "        GOAL             : Create FC network with different specifications \n",
    "        inputs (tensor)  : input tensor\n",
    "        num_layers       : number of layers in FCNet\n",
    "        h_dim  (int)     : number of hidden units\n",
    "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
    "        o_dim  (int)     : number of output units\n",
    "        o_fn             : activation function for output layers (defalut: None)\n",
    "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
    "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
    "    '''\n",
    "    # default active functions (hidden: relu, out: None)\n",
    "    if h_fn is None:\n",
    "        h_fn = tf.nn.relu\n",
    "    if o_fn is None:\n",
    "        o_fn = None\n",
    "\n",
    "    # default initialization functions (weight: Xavier, bias: None)\n",
    "    if w_init is None:\n",
    "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        if num_layers == 1:\n",
    "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "        else:\n",
    "            if layer == 0:\n",
    "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "                if not keep_prob is None:\n",
    "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n",
    "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "                if not keep_prob is None:\n",
    "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "            else: # layer == num_layers-1 (the last layer)\n",
    "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "qq-KZXk5keOM"
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "import random\n",
    "\n",
    "##### USER-DEFINED FUNCTIONS\n",
    "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
    "    '''\n",
    "        mask1 is required to get the contional probability (to calculate the denominator part)\n",
    "        mask1 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(meas_time)[0], num_Event, num_Category]) # for denominator\n",
    "    for i in range(np.shape(meas_time)[0]):\n",
    "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
    "\n",
    "    return mask\n",
    "\n",
    "def f_get_minibatch(mb_size, x, x_mi, label, time, mask1, mask2, mask3):\n",
    "    idx = range(np.shape(x)[0])\n",
    "    idx = random.sample(idx, mb_size)\n",
    "\n",
    "    x_mb     = x[idx, :, :].astype(float)\n",
    "    x_mi_mb  = x_mi[idx, :, :].astype(float)\n",
    "    k_mb     = label[idx, :].astype(float) # censoring(0)/event(1,2,..) label\n",
    "    t_mb     = time[idx, :].astype(float)\n",
    "    m1_mb    = mask1[idx, :, :].astype(float) #fc_mask\n",
    "    m2_mb    = mask2[idx, :, :].astype(float) #fc_mask\n",
    "    m3_mb    = mask3[idx, :].astype(float) #fc_mask\n",
    "    return x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb\n",
    "\n",
    "\n",
    "def f_get_boosted_trainset(x, x_mi, time, label, mask1, mask2, mask3):\n",
    "    _, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(x)[0], 1]), np.cumsum(x[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    total_sample = 0\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        total_sample += np.sum(np.sum(x[i], axis=1) != 0)\n",
    "\n",
    "    new_label          = np.zeros([total_sample, np.shape(label)[1]])\n",
    "    new_time           = np.zeros([total_sample, np.shape(time)[1]])\n",
    "    new_x              = np.zeros([total_sample, np.shape(x)[1], np.shape(x)[2]])\n",
    "    new_x_mi           = np.zeros([total_sample, np.shape(x_mi)[1], np.shape(x_mi)[2]])\n",
    "    new_mask1          = np.zeros([total_sample, np.shape(mask1)[1], np.shape(mask1)[2]])\n",
    "    new_mask2          = np.zeros([total_sample, np.shape(mask2)[1], np.shape(mask2)[2]])\n",
    "    new_mask3          = np.zeros([total_sample, np.shape(mask3)[1]])\n",
    "\n",
    "    tmp_idx = 0\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        max_meas = np.sum(np.sum(x[i], axis=1) != 0)\n",
    "\n",
    "        for t in range(max_meas):\n",
    "            new_label[tmp_idx+t, 0] = label[i,0]\n",
    "            new_time[tmp_idx+t, 0]  = time[i,0]\n",
    "\n",
    "            new_x[tmp_idx+t,:(t+1), :] = x[i,:(t+1), :]\n",
    "            new_x_mi[tmp_idx+t,:(t+1), :] = x_mi[i,:(t+1), :]\n",
    "\n",
    "            new_mask1[tmp_idx+t, :, :] = f_get_fc_mask1(meas_time[i,t].reshape([-1,1]), num_Event, num_Category) #age at the measurement\n",
    "            new_mask2[tmp_idx+t, :, :] = mask2[i, :, :]\n",
    "            new_mask3[tmp_idx+t, :]    = mask3[i, :]\n",
    "\n",
    "        tmp_idx += max_meas\n",
    "        \n",
    "    return(new_x, new_x_mi, new_time, new_label, new_mask1, new_mask2, new_mask3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sRJaXTVTkqDE"
   },
   "outputs": [],
   "source": [
    "# Logging Functions (Not Important)\n",
    "def save_logging(dictionary, log_name):\n",
    "    with open(log_name, 'w') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "eYDmJRwHlJGU"
   },
   "outputs": [],
   "source": [
    "#! pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "H63uoidZJy-z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "l12_qvTbk3Jr"
   },
   "outputs": [],
   "source": [
    "# Evalutation Functions\n",
    "import numpy as np\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "\n",
    "### C(t)-INDEX CALCULATION\n",
    "def c_index(Prediction, Time_survival, Death, Time):\n",
    "    '''\n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky)\n",
    "        - Time_survival   : survival/censoring time\n",
    "        - Death           :\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
    "    '''\n",
    "    N = len(Prediction)\n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
    "  \n",
    "        if (Time_survival[i]<=Time and Death[i]==1):\n",
    "            N_t[i,:] = 1\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "### BRIER-SCORE\n",
    "def brier_score(Prediction, Time_survival, Death, Time):\n",
    "    N = len(Prediction)\n",
    "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
    "\n",
    "    return np.mean((Prediction - y_true)**2)\n",
    "\n",
    "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
    "\n",
    "\n",
    "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
    "def CensoringProb(Y, T):\n",
    "\n",
    "    T = T.reshape([-1]) # (N,) - np array\n",
    "    Y = Y.reshape([-1]) # (N,) - np array\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
    "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
    "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "### C(t)-INDEX CALCULATION\n",
    "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    '''\n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky)\n",
    "        - Time_survival   : survival/censoring time\n",
    "        - Death           :\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
    "    '''\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "\n",
    "    N = len(Prediction)\n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
    "\n",
    "        if len(tmp_idx) == 0:\n",
    "            W = (1./G[1, -1])**2\n",
    "        else:\n",
    "            W = (1./G[1, tmp_idx[0]])**2\n",
    "\n",
    "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
    "\n",
    "        if (T_test[i]<=Time and Y_test[i]==1):\n",
    "            N_t[i,:] = 1.\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "    N = len(Prediction)\n",
    "\n",
    "    W = np.zeros(len(Y_test))\n",
    "    Y_tilde = (T_test > Time).astype(float)\n",
    "\n",
    "    for i in range(N):\n",
    "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
    "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
    "\n",
    "        if len(tmp_idx1) == 0:\n",
    "            G1 = G[1, -1]\n",
    "        else:\n",
    "            G1 = G[1, tmp_idx1[0]]\n",
    "\n",
    "        if len(tmp_idx2) == 0:\n",
    "            G2 = G[1, -1]\n",
    "        else:\n",
    "            G2 = G[1, tmp_idx2[0]]\n",
    "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
    "\n",
    "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
    "\n",
    "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "TmwK1mlHmgHX"
   },
   "outputs": [],
   "source": [
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    '''\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    '''\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sK-sTisytAf"
   },
   "source": [
    "$F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "REcrZt_Wqlqt"
   },
   "outputs": [],
   "source": [
    "data_mode                   = 'PBC2' \n",
    "seed                        = 1234\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = max event/censoring time * 1.2\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (1 + num_features)\n",
    "    x_dim_cont              = dim of continuous features\n",
    "    x_dim_bin               = dim of binary features\n",
    "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
    "'''\n",
    "\n",
    "if data_mode == 'PBC2':\n",
    "    (x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi) = import_dataset(norm_mode = 'standard')\n",
    "    \n",
    "    # This must be changed depending on the datasets, prediction/evaliation times of interest\n",
    "    pred_time = [52, 3*52, 5*52] # prediction time (in months)\n",
    "    eval_time = [12, 36, 60, 120] # months evaluation time (for C-index and Brier-Score)\n",
    "else:\n",
    "    print ('ERROR:  DATA_MODE NOT FOUND !!!')\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "max_length                  = np.shape(data)[1]\n",
    "\n",
    "\n",
    "file_path = '{}'.format(data_mode)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ztBC4EP3qww3"
   },
   "outputs": [],
   "source": [
    "urn_in_mode                = 'ON' #{'ON', 'OFF'}\n",
    "boost_mode                  = 'ON' #{'ON', 'OFF'}\n",
    "\n",
    "##### HYPER-PARAMETERS\n",
    "new_parser = {'mb_size': 32,\n",
    "\n",
    "             'iteration_burn_in': 3000,\n",
    "             'iteration': 25000,\n",
    "\n",
    "             'keep_prob': 0.6,\n",
    "             'lr_train': 1e-4,\n",
    "\n",
    "             'h_dim_RNN': 100,\n",
    "             'h_dim_FC' : 100,\n",
    "             'num_layers_RNN':2,\n",
    "             'num_layers_ATT':2,\n",
    "             'num_layers_CS' :2,\n",
    "\n",
    "             'RNN_type':'LSTM', #{'LSTM', 'GRU'}\n",
    "\n",
    "             'FC_active_fn' : tf.nn.relu,\n",
    "             'RNN_active_fn': tf.nn.tanh,\n",
    "\n",
    "            'reg_W'         : 1e-5,\n",
    "            'reg_W_out'     : 0.,\n",
    "\n",
    "             'alpha' :1.0,\n",
    "             'beta'  :0.1,\n",
    "             'gamma' :1.0\n",
    "}\n",
    "\n",
    "\n",
    "# INPUT DIMENSIONS\n",
    "input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                'x_dim_cont'    : x_dim_cont,\n",
    "                                'x_dim_bin'     : x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "# NETWORK HYPER-PARMETERS\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : new_parser['FC_active_fn'],\n",
    "                                'RNN_active_fn'     : new_parser['RNN_active_fn'],\n",
    "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : new_parser['reg_W_out']\n",
    "                                 }\n",
    "\n",
    "\n",
    "mb_size           = new_parser['mb_size']\n",
    "iteration         = new_parser['iteration']\n",
    "iteration_burn_in = new_parser['iteration_burn_in']\n",
    "\n",
    "keep_prob         = new_parser['keep_prob']\n",
    "lr_train          = new_parser['lr_train']\n",
    "\n",
    "alpha             = new_parser['alpha']\n",
    "beta              = new_parser['beta']\n",
    "gamma             = new_parser['gamma']\n",
    "\n",
    "# SAVE HYPERPARAMETERS\n",
    "log_name = file_path + '/hyperparameters_log.txt'\n",
    "save_logging(new_parser, log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "aMmXZuo5rFoB"
   },
   "outputs": [],
   "source": [
    "### TRAINING-TESTING SPLIT\n",
    "(tr_data,te_data, tr_data_mi, te_data_mi, tr_time,te_time, tr_label,te_label, \n",
    " tr_mask1,te_mask1, tr_mask2,te_mask2, tr_mask3,te_mask3) = train_test_split(data, data_mi, time, label, mask1, mask2, mask3, test_size=0.2, random_state=seed) \n",
    "\n",
    "(tr_data,va_data, tr_data_mi, va_data_mi, tr_time,va_time, tr_label,va_label, \n",
    " tr_mask1,va_mask1, tr_mask2,va_mask2, tr_mask3,va_mask3) = train_test_split(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3, test_size=0.2, random_state=seed) \n",
    "\n",
    "if boost_mode == 'ON':\n",
    "    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz0fEUWdrK6O",
    "outputId": "a46aa157-ceaa-43ed-d89c-9adb143d82a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "BURN-IN TRAINING ...\n",
      "itr: 1000 | loss: 311.6709\n",
      "itr: 2000 | loss: 159.9081\n",
      "itr: 3000 | loss: 198.2345\n",
      "MAIN TRAINING ...\n",
      "itr: 1000 | loss: 183.1167\n",
      "updated.... average c-index = 0.7391\n",
      "itr: 2000 | loss: 97.1676\n",
      "updated.... average c-index = 0.7986\n",
      "itr: 3000 | loss: 84.7476\n",
      "updated.... average c-index = 0.8343\n",
      "itr: 4000 | loss: 135.4168\n",
      "itr: 5000 | loss: 41.5561\n",
      "itr: 6000 | loss: 110.4650\n",
      "updated.... average c-index = 0.8530\n",
      "itr: 7000 | loss: 49.9428\n",
      "itr: 8000 | loss: 148.4012\n",
      "itr: 9000 | loss: 57.5544\n",
      "itr: 10000 | loss: 108.6998\n",
      "itr: 11000 | loss: 38.5790\n",
      "itr: 12000 | loss: 37.7476\n",
      "itr: 13000 | loss: 58.6359\n",
      "itr: 14000 | loss: 98.4904\n",
      "itr: 15000 | loss: 65.6847\n",
      "itr: 16000 | loss: 61.6053\n",
      "itr: 17000 | loss: 86.1334\n",
      "itr: 18000 | loss: 85.1382\n",
      "itr: 19000 | loss: 211.7210\n",
      "itr: 20000 | loss: 90.1041\n",
      "itr: 21000 | loss: 67.0435\n",
      "itr: 22000 | loss: 50.9932\n",
      "itr: 23000 | loss: 66.8991\n",
      "itr: 24000 | loss: 172.3137\n",
      "itr: 25000 | loss: 186.9820\n"
     ]
    }
   ],
   "source": [
    "##### CREATE DYNAMIC-DEEPFHT NETWORK\n",
    "tf.reset_default_graph()\n",
    "\n",
    "burn_in_mode = \"ON\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    " \n",
    "### TRAINING - BURN-IN\n",
    "if burn_in_mode == 'ON':\n",
    "    print( \"BURN-IN TRAINING ...\")\n",
    "    for itr in range(iteration_burn_in):\n",
    "        x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
    "        DATA = (x_mb, k_mb, t_mb)\n",
    "        MISSING = (x_mi_mb)\n",
    "\n",
    "        _, loss_curr = model.train_burn_in(DATA, MISSING, keep_prob, lr_train)\n",
    "\n",
    "        if (itr+1)%1000 == 0:\n",
    "            print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
    "\n",
    "\n",
    "### TRAINING - MAIN\n",
    "print( \"MAIN TRAINING ...\")\n",
    "min_valid = 0.5\n",
    "\n",
    "for itr in range(iteration):\n",
    "    x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
    "    DATA = (x_mb, k_mb, t_mb)\n",
    "    MASK = (m1_mb, m2_mb, m3_mb)\n",
    "    MISSING = (x_mi_mb)\n",
    "    PARAMETERS = (alpha, beta, gamma)\n",
    "\n",
    "    _, loss_curr = model.train(DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train)\n",
    "\n",
    "    if (itr+1)%1000 == 0:\n",
    "        print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
    "\n",
    "    ### VALIDATION  (based on average C-index of our interest)\n",
    "    if (itr+1)%1000 == 0:        \n",
    "        risk_all = f_get_risk_predictions(sess, model, va_data, va_data_mi, pred_time, eval_time)\n",
    "        \n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            pred_horizon = int(p_time)\n",
    "            val_result1 = np.zeros([num_Event, len(eval_time)])\n",
    "            \n",
    "            for t, t_time in enumerate(eval_time):                \n",
    "                eval_horizon = int(t_time) + pred_horizon\n",
    "                for k in range(num_Event):\n",
    "                    val_result1[k, t] = c_index(risk_all[k][:, p, t], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            \n",
    "            if p == 0:\n",
    "                val_final1 = val_result1\n",
    "            else:\n",
    "                val_final1 = np.append(val_final1, val_result1, axis=0)\n",
    "\n",
    "        tmp_valid = np.mean(val_final1)\n",
    "\n",
    "        if tmp_valid >  min_valid:\n",
    "            min_valid = tmp_valid\n",
    "            saver.save(sess, file_path + '/model')\n",
    "            print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbiCw72FrO32",
    "outputId": "94bd57fb-6048-4e05-8a3a-3af878d82a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from PBC2/model\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, file_path + '/model')\n",
    "\n",
    "# risk_all = f_get_risk_predictions(sess, model, te_data, te_data_mi, pred_time, eval_time)\n",
    "\n",
    "# for p, p_time in enumerate(pred_time):\n",
    "#     pred_horizon = int(p_time)\n",
    "#     result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "#     for t, t_time in enumerate(eval_time):                \n",
    "#         eval_horizon = int(t_time) + pred_horizon\n",
    "#         for k in range(num_Event):\n",
    "#             result1[k, t] = c_index(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "#             result2[k, t] = brier_score(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    \n",
    "#     if p == 0:\n",
    "#         final1, final2 = result1, result2\n",
    "#     else:\n",
    "#         final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "        \n",
    "        \n",
    "# row_header = []\n",
    "# for p_time in pred_time:\n",
    "#     for t in range(num_Event):\n",
    "#         row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "            \n",
    "# col_header = []\n",
    "# for t_time in eval_time:\n",
    "#     col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "# # c-index result\n",
    "# df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# # brier-score result\n",
    "# df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "# ### PRINT RESULTS\n",
    "# print('========================================================')\n",
    "# print('--------------------------------------------------------')\n",
    "# print('- C-INDEX: ')\n",
    "# print(df1)\n",
    "# print('--------------------------------------------------------')\n",
    "# print('- BRIER-SCORE: ')\n",
    "# print(df2)\n",
    "# print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zCObDC_xRRT",
    "outputId": "c5fe0924-cdee-4ed8-d4b3-69606b1f86f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "print(pred_horizon)\n",
    "predict_260 = _f_get_pred(sess, model, data, data_mi, pred_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2kRuK3YQjvr",
    "outputId": "bc23baa1-194b-4f77-aee8-bf9c583bee1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 892)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(predict_260[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "v68ipKOMQ0SH"
   },
   "outputs": [],
   "source": [
    "# deep_hit_predictions = {}\n",
    "\n",
    "# for patient in range(312):\n",
    "#   deep_hit_predictions[patient] = predict_260[patient][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "528K-JcriUzK",
    "outputId": "124b510d-4f52-4126-be38-95ea472a779c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.7992091e-03, 2.7524326e-03, 2.8156841e-03, ...,\n",
       "         8.3824701e-04, 2.3760856e-03, 4.3087807e-03]],\n",
       "\n",
       "       [[8.7626040e-04, 2.5904554e-04, 2.9273194e-04, ...,\n",
       "         3.5812636e-06, 4.0256229e-04, 5.4978388e-03]],\n",
       "\n",
       "       [[1.8870189e-03, 7.9071266e-04, 8.6270703e-04, ...,\n",
       "         3.6906898e-05, 9.8068605e-04, 6.0865926e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.1410485e-03, 3.7984858e-04, 4.2054307e-04, ...,\n",
       "         7.6782244e-06, 5.4208544e-04, 5.7091471e-03]],\n",
       "\n",
       "       [[4.1619487e-04, 9.0805544e-05, 1.0269510e-04, ...,\n",
       "         3.8769494e-07, 1.6667516e-04, 4.6556885e-03]],\n",
       "\n",
       "       [[3.2109753e-03, 1.8020622e-03, 1.8888137e-03, ...,\n",
       "         2.2493744e-04, 1.8307484e-03, 5.9143635e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data, data_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBaj23tiiagH",
    "outputId": "c4d819fc-eefc-4191-9170-6b50fe6c43bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 1, 892)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(data, data_mi)\n",
    "np.shape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFY2wu7PR4zJ"
   },
   "outputs": [],
   "source": [
    "# deep_hit_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "DGiUVYPHR-hV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "TEiFhJ1HSMb4",
    "outputId": "c3376d86-ecaf-4301-cef9-a6b8798ed78b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea6c3e14d0>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.step(range(892), deep_hit_predictions[1], where=\"post\", label=str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYs38_NgSW3f",
    "outputId": "bae9e6a1-e476-496e-e841-bf905388db32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024337883"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_hit_predictions[5][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Z7cbB1CDTVRH"
   },
   "outputs": [],
   "source": [
    "def surv_probability(patient,t):\n",
    "  return np.sum(predictions[patient][0][t:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "-gGWenb6g0KE"
   },
   "outputs": [],
   "source": [
    "event_times = [  8.,  11.,  16.,  20.,  26.,  27.,  29.,  30.,  31.,  32.,  38.,\n",
    "        44.,  47.,  50.,  56.,  74.,  76.,  79.,  89.,  96.,  99., 101.,\n",
    "       105., 109., 112., 114., 118., 120., 122., 125., 127., 129., 133.,\n",
    "       134., 135., 139., 143., 145., 148., 152., 154., 155., 165., 166.,\n",
    "       167., 170., 173., 185., 186., 193., 194., 201., 204., 205., 206.,\n",
    "       213., 215., 219., 220., 235., 236., 239., 241., 248., 255., 261.,\n",
    "       263., 264., 267., 271., 275., 278., 285., 287., 292., 293., 297.,\n",
    "       298., 303., 317., 322., 328., 332., 335., 336., 340., 345., 353.,\n",
    "       357., 362., 364., 368., 370., 395., 399., 411., 438., 440., 441.,\n",
    "       459., 462., 468., 482., 484., 489., 491., 511., 525., 536., 547.,\n",
    "       549., 556., 597.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-y70wSci9xs",
    "outputId": "6d852af2-b9b7-47cc-c195-aa88a1637bde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34562907"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv_probability(0,597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "3oH_bQIQhcQs"
   },
   "outputs": [],
   "source": [
    "deep_hit_predictions = {}\n",
    "for patient in range(312):\n",
    "  surv_array = np.zeros(np.shape(event_times))\n",
    "  i = 0\n",
    "  for time in event_times:\n",
    "    surv_array[i] = surv_probability(patient,np.int(time))\n",
    "    i += 1\n",
    "  deep_hit_predictions[patient] = surv_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLZlE7ihh3LI"
   },
   "outputs": [],
   "source": [
    "deep_hit_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "tlM7MHBlkFp_",
    "outputId": "f8e916f8-b89b-4777-86a7-242a2e06e4d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea6c060750>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.step(event_times, deep_hit_predictions[1], where=\"post\", label=str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "16ddh76ykchS"
   },
   "outputs": [],
   "source": [
    "deep_hit_predictions['event_times'] = event_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "L6VVQnlhkMGk"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( deep_hit_predictions, open( \"patient_surv_functions/deep_hit.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h91qyNDWkl0H",
    "outputId": "319af1bb-2f32-4c3b-adad-150131e18e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97602594 0.96717358 0.95355284 0.94234091 0.92675257 0.9244405\n",
      " 0.91750813 0.91242731 0.91051555 0.90727079 0.89505213 0.88507223\n",
      " 0.88002944 0.87186813 0.86114955 0.8357904  0.83045012 0.82405996\n",
      " 0.81120968 0.80207276 0.79629648 0.79097235 0.78794271 0.77694178\n",
      " 0.76893604 0.76464844 0.75772178 0.75696939 0.75580412 0.7478587\n",
      " 0.74366575 0.73939764 0.73311806 0.73283708 0.72857118 0.72399569\n",
      " 0.71779931 0.71265054 0.70716488 0.70558751 0.70511162 0.70491296\n",
      " 0.69829142 0.69440997 0.69066381 0.68676567 0.68585706 0.66798365\n",
      " 0.66774607 0.66617465 0.66601312 0.66087282 0.66037691 0.65542191\n",
      " 0.65508372 0.64445496 0.63923347 0.63372701 0.62932265 0.62202078\n",
      " 0.62186491 0.61918736 0.60933143 0.60439444 0.59866118 0.59301484\n",
      " 0.59263557 0.58779556 0.58339906 0.57868433 0.57360852 0.56848812\n",
      " 0.56746155 0.56297612 0.55306774 0.55285037 0.54781502 0.54327154\n",
      " 0.54262376 0.53533965 0.52383137 0.51332808 0.50998843 0.50641108\n",
      " 0.50629169 0.50077379 0.49548975 0.48891664 0.48846003 0.48227093\n",
      " 0.48196381 0.47633249 0.47075829 0.46098554 0.46039361 0.45367855\n",
      " 0.43858564 0.43827495 0.43245307 0.41797081 0.41255349 0.41143408\n",
      " 0.39756525 0.39720827 0.39069939 0.39036581 0.38658822 0.37797236\n",
      " 0.36992252 0.36208034 0.36163187 0.35453311 0.34562907]\n"
     ]
    }
   ],
   "source": [
    "deep_hit_predictions = pickle.load( open( \"patient_surv_functions/deep_hit.p\", \"rb\" ) )\n",
    "print(deep_hit_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwcDBCQCkzsB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepHIT Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
