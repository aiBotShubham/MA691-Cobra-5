{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvHXhyzea-mx",
    "outputId": "bed9f54c-15c1-4b63-d556-09f2a3c04c9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbnqQmUVbhtH"
   },
   "outputs": [],
   "source": [
    "# Dataset Functions\n",
    "##### USER-DEFINED FUNCTIONS\n",
    "def f_get_Normalization(X, norm_mode):    \n",
    "    num_Patient, num_Feature = np.shape(X)\n",
    "    \n",
    "    if norm_mode == 'standard': #zero mean unit variance\n",
    "        for j in range(num_Feature):\n",
    "            if np.nanstd(X[:,j]) != 0:\n",
    "                X[:,j] = (X[:,j] - np.nanmean(X[:, j]))/np.nanstd(X[:,j])\n",
    "            else:\n",
    "                X[:,j] = (X[:,j] - np.nanmean(X[:, j]))\n",
    "    elif norm_mode == 'normal': #min-max normalization\n",
    "        for j in range(num_Feature):\n",
    "            X[:,j] = (X[:,j] - np.nanmin(X[:,j]))/(np.nanmax(X[:,j]) - np.nanmin(X[:,j]))\n",
    "    else:\n",
    "        print(\"INPUT MODE ERROR!\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
    "    '''\n",
    "        mask3 is required to get the contional probability (to calculate the denominator part)\n",
    "        mask3 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(meas_time)[0], num_Event, num_Category]) # for denominator\n",
    "    for i in range(np.shape(meas_time)[0]):\n",
    "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
    "    '''\n",
    "        mask4 is required to get the log-likelihood loss \n",
    "        mask4 size is [N, num_Event, num_Category]\n",
    "            if not censored : one element = 1 (0 elsewhere)\n",
    "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
    "    for i in range(np.shape(time)[0]):\n",
    "        if label[i,0] != 0:  #not censored\n",
    "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
    "        else: #label[i,2]==0: censored\n",
    "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask3(time, meas_time, num_Category):\n",
    "    '''\n",
    "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
    "        mask5 size is [N, num_Category]. \n",
    "        - For longitudinal measurements:\n",
    "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
    "             denom is not needed since comparing is done over the same denom\n",
    "        - For single measurement:\n",
    "             1's from start to the event time(inclusive)\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
    "    if np.shape(meas_time):  #lonogitudinal measurements \n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t1 = int(meas_time[i, 0]) # last measurement time\n",
    "            t2 = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    else:                    #single measurement\n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "##### TRANSFORMING DATA\n",
    "def f_construct_dataset(df, feat_list):\n",
    "    '''\n",
    "        id   : patient indicator\n",
    "        tte  : time-to-event or time-to-censoring\n",
    "            - must be synchronized based on the reference time\n",
    "        times: time at which observations are measured\n",
    "            - must be synchronized based on the reference time (i.e., times start from 0)\n",
    "        label: event/censoring information\n",
    "            - 0: censoring\n",
    "            - 1: event type 1\n",
    "            - 2: event type 2\n",
    "            ...\n",
    "    '''\n",
    "\n",
    "    grouped  = df.groupby(['id'])\n",
    "    id_list  = pd.unique(df['id'])\n",
    "    max_meas = np.max(grouped.count())[0]\n",
    "\n",
    "    data     = np.zeros([len(id_list), max_meas, len(feat_list)+1])\n",
    "    pat_info = np.zeros([len(id_list), 5])\n",
    "\n",
    "    for i, tmp_id in enumerate(id_list):\n",
    "        tmp = grouped.get_group(tmp_id).reset_index(drop=True)\n",
    "\n",
    "        pat_info[i,4] = tmp.shape[0]                                   #number of measurement\n",
    "        pat_info[i,3] = np.max(tmp['times'])     #last measurement time\n",
    "        pat_info[i,2] = tmp['label'][0]      #cause\n",
    "        pat_info[i,1] = tmp['tte'][0]         #time_to_event\n",
    "        pat_info[i,0] = tmp['id'][0]      \n",
    "\n",
    "        data[i, :int(pat_info[i, 4]), 1:]  = tmp[feat_list]\n",
    "        data[i, :int(pat_info[i, 4]-1), 0] = np.diff(tmp['times'])\n",
    "    \n",
    "    return pat_info, data\n",
    "\n",
    "\n",
    "def import_dataset(norm_mode = 'standard'):\n",
    "\n",
    "    df_                = pd.read_csv('pbc2_cleaned.csv')\n",
    "\n",
    "    bin_list           = ['drug', 'sex', 'ascites', 'hepatomegaly', 'spiders']\n",
    "    cont_list          = ['age', 'edema', 'serBilir', 'serChol', 'albumin', 'alkaline', 'SGOT', 'platelets', 'prothrombin', 'histologic']\n",
    "    feat_list          = cont_list + bin_list\n",
    "    df_                = df_[['id', 'tte', 'times', 'label']+feat_list]\n",
    "    df_org_            = df_.copy(deep=True)\n",
    "\n",
    "    df_[cont_list]     = f_get_Normalization(np.asarray(df_[cont_list]).astype(float), norm_mode)\n",
    "\n",
    "    pat_info, data     = f_construct_dataset(df_, feat_list)\n",
    "    _, data_org        = f_construct_dataset(df_org_, feat_list)\n",
    "\n",
    "    data_mi                  = np.zeros(np.shape(data))\n",
    "    data_mi[np.isnan(data)]  = 1\n",
    "    data_org[np.isnan(data)] = 0\n",
    "    data[np.isnan(data)]     = 0 \n",
    "\n",
    "    x_dim           = np.shape(data)[2] # 1 + x_dim_cont + x_dim_bin (including delta)\n",
    "    x_dim_cont      = len(cont_list)\n",
    "    x_dim_bin       = len(bin_list) \n",
    "\n",
    "    last_meas       = pat_info[:,[3]]  #pat_info[:, 3] contains age at the last measurement\n",
    "    label           = pat_info[:,[2]]  #two competing risks\n",
    "    time            = pat_info[:,[1]]  #age when event occurred\n",
    "\n",
    "    num_Category    = int(np.max(pat_info[:, 1]) * 1.2) #or specifically define larger than the max tte\n",
    "    num_Event       = len(np.unique(label)) - 1\n",
    "\n",
    "    if num_Event == 1:\n",
    "        label[np.where(label!=0)] = 1 #make single risk\n",
    "\n",
    "    mask1           = f_get_fc_mask1(last_meas, num_Event, num_Category)\n",
    "    mask2           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
    "    mask3           = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "    DIM             = (x_dim, x_dim_cont, x_dim_bin)\n",
    "    DATA            = (data, time, label)\n",
    "    # DATA            = (data, data_org, time, label)\n",
    "    MASK            = (mask1, mask2, mask3)\n",
    "\n",
    "    return DIM, DATA, MASK, data_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XK4ZMw5m7Rk",
    "outputId": "59fad87e-1ae8-4f5a-8a26-fb99814206d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 1.13.1\n",
      "Uninstalling tensorflow-1.13.1:\n",
      "  Would remove:\n",
      "    /usr/local/bin/freeze_graph\n",
      "    /usr/local/bin/saved_model_cli\n",
      "    /usr/local/bin/tensorboard\n",
      "    /usr/local/bin/tf_upgrade_v2\n",
      "    /usr/local/bin/tflite_convert\n",
      "    /usr/local/bin/toco\n",
      "    /usr/local/bin/toco_from_protos\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow-1.13.1.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
      "Proceed (y/n)? n\n"
     ]
    }
   ],
   "source": [
    "# ! pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umDVWr7apNq4",
    "outputId": "58604b56-96aa-4c50-e16b-f9ecb027e835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.41.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.8.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrEPncdlhuTU"
   },
   "outputs": [],
   "source": [
    "# Longitudional Model Functions\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x + _EPSILON)\n",
    "\n",
    "def div(x, y):\n",
    "    return tf.div(x, (y + _EPSILON))\n",
    "\n",
    "def get_seq_length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    tmp_length = tf.reduce_sum(used, 1)\n",
    "    tmp_length = tf.cast(tmp_length, tf.int32)\n",
    "    return tmp_length\n",
    "\n",
    "\n",
    "class Model_Longitudinal_Attention:\n",
    "    def __init__(self, sess, name, input_dims, network_settings):\n",
    "        self.sess               = sess\n",
    "        self.name               = name\n",
    "\n",
    "        # INPUT DIMENSIONS\n",
    "        self.x_dim              = input_dims['x_dim']\n",
    "        self.x_dim_cont         = input_dims['x_dim_cont']\n",
    "        self.x_dim_bin          = input_dims['x_dim_bin']\n",
    "\n",
    "        self.num_Event          = input_dims['num_Event']\n",
    "        self.num_Category       = input_dims['num_Category']\n",
    "        self.max_length         = input_dims['max_length']\n",
    "\n",
    "        # NETWORK HYPER-PARMETERS\n",
    "        self.h_dim1             = network_settings['h_dim_RNN']\n",
    "        self.h_dim2             = network_settings['h_dim_FC']\n",
    "        self.num_layers_RNN     = network_settings['num_layers_RNN']\n",
    "        self.num_layers_ATT     = network_settings['num_layers_ATT']\n",
    "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
    "\n",
    "        self.RNN_type           = network_settings['RNN_type']\n",
    "\n",
    "        self.FC_active_fn       = network_settings['FC_active_fn']\n",
    "        self.RNN_active_fn      = network_settings['RNN_active_fn']\n",
    "        self.initial_W          = network_settings['initial_W']\n",
    "        \n",
    "        self.reg_W              = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W'])\n",
    "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W_out'])\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            #### PLACEHOLDER DECLARATION\n",
    "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "\n",
    "            self.lr_rate     = tf.placeholder(tf.float32)\n",
    "            self.keep_prob   = tf.placeholder(tf.float32)                                                      #keeping rate\n",
    "            self.a           = tf.placeholder(tf.float32)\n",
    "            self.b           = tf.placeholder(tf.float32)\n",
    "            self.c           = tf.placeholder(tf.float32)\n",
    "\n",
    "            self.x           = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])\n",
    "            self.x_mi        = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])           #this is the missing indicator (including for cont. & binary) (includes delta)\n",
    "            self.k           = tf.placeholder(tf.float32, shape=[None, 1])                                     #event/censoring label (censoring:0)\n",
    "            self.t           = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for denominator\n",
    "            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for Loss 1\n",
    "            self.fc_mask3    = tf.placeholder(tf.float32, shape=[None, self.num_Category])                     #for Loss 2\n",
    "\n",
    "            \n",
    "            seq_length     = get_seq_length(self.x)\n",
    "            tmp_range      = tf.expand_dims(tf.range(0, self.max_length, 1), axis=0)\n",
    "            \n",
    "            self.rnn_mask1 = tf.cast(tf.less_equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32)            \n",
    "            self.rnn_mask2 = tf.cast(tf.equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32) \n",
    "            \n",
    "            \n",
    "            ### DEFINE LOOP FUNCTION FOR RAW_RNN w/ TEMPORAL ATTENTION\n",
    "            def loop_fn_att(time, cell_output, cell_state, loop_state):\n",
    "\n",
    "                emit_output = cell_output \n",
    "\n",
    "                if cell_output is None:  # time == 0\n",
    "                    next_cell_state = cell.zero_state(self.mb_size, tf.float32)\n",
    "                    next_loop_state = loop_state_ta\n",
    "                else:\n",
    "                    next_cell_state = cell_state\n",
    "                    tmp_h = create_concat_state(next_cell_state, self.num_layers_RNN, self.RNN_type)\n",
    "\n",
    "                    e = create_FCNet(tf.concat([tmp_h, all_last], axis=1), self.num_layers_ATT, self.h_dim2, \n",
    "                                           tf.nn.tanh, 1, None, self.initial_W, keep_prob=self.keep_prob)\n",
    "                    e = tf.exp(e)\n",
    "\n",
    "                    next_loop_state = (loop_state[0].write(time-1, e),                # save att power (e_{j})\n",
    "                                       loop_state[1].write(time-1, tmp_h))  # save all the hidden states\n",
    "\n",
    "                # elements_finished = (time >= seq_length)\n",
    "                elements_finished = (time >= self.max_length-1)\n",
    "\n",
    "                #this gives the break-point (no more recurrence after the max_length)\n",
    "                finished = tf.reduce_all(elements_finished)    \n",
    "                next_input = tf.cond(finished, lambda: tf.zeros([self.mb_size, 2*self.x_dim], dtype=tf.float32),  # [x_hist, mi_hist]\n",
    "                                               lambda: inputs_ta.read(time))\n",
    "\n",
    "                return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "\n",
    "            \n",
    "            # divide into the last x and previous x's\n",
    "            x_last = tf.slice(self.x, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
    "            x_last = tf.reshape(x_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
    "\n",
    "            x_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
    "            x_last = tf.slice(x_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
    "            x_hist = self.x * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
    "            x_hist = tf.slice(x_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
    "\n",
    "            # do same thing for missing indicator\n",
    "            mi_last = tf.slice(self.x_mi, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
    "            mi_last = tf.reshape(mi_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
    "\n",
    "            mi_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x_mi, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
    "            mi_last = tf.slice(mi_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
    "            mi_hist = self.x_mi * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
    "            mi_hist = tf.slice(mi_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
    "\n",
    "            all_hist = tf.concat([x_hist, mi_hist], axis=2)\n",
    "            all_last = tf.concat([x_last, mi_last], axis=1)\n",
    "\n",
    "\n",
    "            #extract inputs for the temporal attention: mask (to incorporate only the measured time) and x_{M}\n",
    "            seq_length     = get_seq_length(x_hist)\n",
    "            rnn_mask_att   = tf.cast(tf.not_equal(tf.reduce_sum(x_hist, reduction_indices=2), 0), dtype=tf.float32)  #[mb_size, max_length-1], 1:measurements 0:no measurements\n",
    "            \n",
    "\n",
    "            ##### SHARED SUBNETWORK: RNN w/ TEMPORAL ATTENTION\n",
    "            #change the input tensor to TensorArray format with [max_length, mb_size, x_dim]\n",
    "            inputs_ta = tf.TensorArray(dtype=tf.float32, size=self.max_length-1).unstack(_transpose_batch_time(all_hist), name = 'Shared_Input')\n",
    "\n",
    "\n",
    "            #create a cell with RNN hyper-parameters (RNN types, #layers, #nodes, activation functions, keep proability)\n",
    "            cell = create_rnn_cell(self.h_dim1, self.num_layers_RNN, self.keep_prob, \n",
    "                                         self.RNN_type, self.RNN_active_fn)\n",
    "\n",
    "            #define the loop_state TensorArray for information from rnn time steps\n",
    "            loop_state_ta = (tf.TensorArray(size=self.max_length-1, dtype=tf.float32),  #e values (e_{j})\n",
    "                             tf.TensorArray(size=self.max_length-1, dtype=tf.float32))  #hidden states (h_{j})\n",
    "            \n",
    "            rnn_outputs_ta, self.rnn_final_state, loop_state_ta = tf.nn.raw_rnn(cell, loop_fn_att)\n",
    "            #rnn_outputs_ta  : TensorArray\n",
    "            #rnn_final_state : Tensor\n",
    "            #rnn_states_ta   : (TensorArray, TensorArray)\n",
    "\n",
    "            rnn_outputs = _transpose_batch_time(rnn_outputs_ta.stack())\n",
    "            # rnn_outputs =  tf.reshape(rnn_outputs, [-1, self.max_length-1, self.h_dim1])\n",
    "\n",
    "            rnn_states  = _transpose_batch_time(loop_state_ta[1].stack())\n",
    "\n",
    "            att_weight  = _transpose_batch_time(loop_state_ta[0].stack()) #e_{j}\n",
    "            att_weight  = tf.reshape(att_weight, [-1, self.max_length-1]) * rnn_mask_att # masking to set 0 for the unmeasured e_{j}\n",
    "\n",
    "            #get a_{j} = e_{j}/sum_{l=1}^{M-1}e_{l}\n",
    "            self.att_weight  = div(att_weight,(tf.reduce_sum(att_weight, axis=1, keepdims=True) + _EPSILON)) #softmax (tf.exp is done, previously)\n",
    "\n",
    "            # 1) expand att_weight to hidden state dimension, 2) c = \\sum_{j=1}^{M} a_{j} x h_{j}\n",
    "            self.context_vec = tf.reduce_sum(tf.tile(tf.reshape(self.att_weight, [-1, self.max_length-1, 1]), [1, 1, self.num_layers_RNN*self.h_dim1]) * rnn_states, axis=1)\n",
    "\n",
    "\n",
    "            self.z_mean      = FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_mean1\")\n",
    "            self.z_std       = tf.exp(FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_std1\"))\n",
    "\n",
    "            epsilon          = tf.random_normal([self.mb_size, self.max_length-1, self.x_dim], mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "            self.z           = self.z_mean + self.z_std * epsilon\n",
    "\n",
    "            \n",
    "            ##### CS-SPECIFIC SUBNETWORK w/ FCNETS \n",
    "            inputs = tf.concat([x_last, self.context_vec], axis=1)\n",
    "\n",
    "\n",
    "            #1 layer for combining inputs\n",
    "            h = FC_Net(inputs, self.h_dim2, activation_fn=self.FC_active_fn, weights_initializer=self.initial_W, scope=\"Layer1\")\n",
    "            h = tf.nn.dropout(h, keep_prob=self.keep_prob)\n",
    "\n",
    "            # (num_layers_CS-1) layers for cause-specific (num_Event subNets)\n",
    "            out = []\n",
    "            for _ in range(self.num_Event):\n",
    "                cs_out = create_FCNet(h, (self.num_layers_CS), self.h_dim2, self.FC_active_fn, self.h_dim2, self.FC_active_fn, self.initial_W, self.reg_W, self.keep_prob)\n",
    "                out.append(cs_out)\n",
    "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
    "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim2])\n",
    "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
    "\n",
    "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
    "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
    "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
    "\n",
    "\n",
    "            ##### GET LOSS FUNCTIONS\n",
    "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
    "            self.loss_Ranking()             #get loss2: Ranking loss\n",
    "            self.loss_RNN_Prediction()      #get loss3: RNN prediction loss\n",
    "\n",
    "            self.LOSS_TOTAL     = self.a*self.LOSS_1 + self.b*self.LOSS_2 + self.c*self.LOSS_3 + tf.losses.get_regularization_loss()\n",
    "            self.LOSS_BURNIN    = self.LOSS_3 + tf.losses.get_regularization_loss()\n",
    "\n",
    "            self.solver         = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
    "            self.solver_burn_in = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_BURNIN)\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
    "    def loss_Log_Likelihood(self):\n",
    "        sigma3 = tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "        I_1 = tf.sign(self.k)\n",
    "        denom = 1 - tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True) # make subject specific denom.\n",
    "        denom = tf.clip_by_value(denom, tf.cast(_EPSILON, dtype=tf.float32), tf.cast(1.-_EPSILON, dtype=tf.float32))\n",
    "\n",
    "        #for uncenosred: log P(T=t,K=k|x,Y,t>t_M)\n",
    "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
    "        tmp1 = I_1 * log(div(tmp1,denom))\n",
    "\n",
    "        #for censored: log \\sum P(T>t|x,Y,t>t_M)\n",
    "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
    "        tmp2 = (1. - I_1) * log(div(tmp2,denom))\n",
    "\n",
    "        self.LOSS_1 = - tf.reduce_mean(tmp1 + sigma3*tmp2)\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
    "    def loss_Ranking(self):\n",
    "        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
    "\n",
    "        eta = []\n",
    "        for e in range(self.num_Event):\n",
    "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
    "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
    "            I_2 = tf.diag(tf.squeeze(I_2))\n",
    "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
    "\n",
    "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask3)) #no need to divide by each individual dominator\n",
    "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
    "\n",
    "            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n",
    "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
    "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
    "\n",
    "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
    "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
    "\n",
    "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
    "\n",
    "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keepdims=True)\n",
    "\n",
    "            eta.append(tmp_eta)\n",
    "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
    "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keepdims=True)\n",
    "\n",
    "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
    "\n",
    "\n",
    "    ### LOSS-FUNCTION 3 -- RNN prediction loss\n",
    "    def loss_RNN_Prediction(self):\n",
    "        tmp_x  = tf.slice(self.x, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
    "        tmp_mi = tf.slice(self.x_mi, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
    "\n",
    "        tmp_mask1  = tf.tile(tf.expand_dims(self.rnn_mask1, axis=2), [1,1,self.x_dim]) #for hisotry (1...J-1)\n",
    "        tmp_mask1  = tmp_mask1[:, :(self.max_length-1), :] \n",
    "\n",
    "        zeta = tf.reduce_mean(tf.reduce_sum(tmp_mask1 * (1. - tmp_mi) * tf.pow(self.z - tmp_x, 2), reduction_indices=1))  #loss calculated for selected features.\n",
    "\n",
    "        self.LOSS_3 = zeta\n",
    "\n",
    " \n",
    "    def get_cost(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb)        = DATA\n",
    "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
    "        (x_mi_mb)                 = MISSING\n",
    "        (alpha, beta, gamma)      = PARAMETERS\n",
    "        return self.sess.run(self.LOSS_TOTAL, \n",
    "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
    "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
    "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "\n",
    "    def train(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb)        = DATA\n",
    "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
    "        (x_mi_mb)                 = MISSING\n",
    "        (alpha, beta, gamma)      = PARAMETERS\n",
    "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
    "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb,\n",
    "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
    "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "    \n",
    "    def train_burn_in(self, DATA, MISSING, keep_prob, lr_train):\n",
    "        (x_mb, k_mb, t_mb)        = DATA\n",
    "        (x_mi_mb)                 = MISSING\n",
    "\n",
    "        return self.sess.run([self.solver_burn_in, self.LOSS_3], \n",
    "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
    "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
    "    \n",
    "    def predict(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_z(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.z, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_rnnstate(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.rnn_final_state, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_att(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.att_weight, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def predict_context_vec(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run(self.context_vec, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
    "\n",
    "    def get_z_mean_and_std(self, x_test, x_mi_test, keep_prob=1.0):\n",
    "        return self.sess.run([self.z_mean, self.z_std], feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16KxNgrojjDa"
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "\n",
    "\n",
    "### CONSTRUCT MULTICELL FOR MULTI-LAYER RNNS\n",
    "def create_rnn_cell(num_units, num_layers, keep_prob, RNN_type, activation_fn): \n",
    "    '''\n",
    "        GOAL         : create multi-cell (including a single cell) to construct multi-layer RNN\n",
    "        num_units    : number of units in each layer\n",
    "        num_layers   : number of layers in MulticellRNN\n",
    "        keep_prob    : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
    "        RNN_type     : either 'LSTM' or 'GRU'\n",
    "    '''\n",
    "    if activation_fn == 'None':\n",
    "        activation_fn = tf.nn.tanh\n",
    "\n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        if RNN_type == 'GRU':\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units, activation=activation_fn)\n",
    "        elif RNN_type == 'LSTM':\n",
    "            cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn, state_is_tuple=True)\n",
    "            # cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn)\n",
    "        if not keep_prob is None:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob) # state_keep_prob=keep_prob\n",
    "        cells.append(cell)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    return cell\n",
    "\n",
    "\n",
    "### EXTRACT STATE OUTPUT OF MULTICELL-RNNS\n",
    "def create_concat_state(state, num_layers, RNN_type, BiRNN=None):\n",
    "    '''\n",
    "        GOAL\t     : concatenate the tuple-type tensor (state) into a single tensor\n",
    "        state        : input state is a tuple ofo MulticellRNN (i.e. output of MulticellRNN)\n",
    "                       consist of only hidden states h for GRU and hidden states c and h for LSTM\n",
    "        num_layers   : number of layers in MulticellRNN\n",
    "        RNN_type     : either 'LSTM' or 'GRU'\n",
    "    '''\n",
    "    for i in range(num_layers):\n",
    "        if BiRNN != None:\n",
    "            if RNN_type == 'LSTM':\n",
    "                tmp = tf.concat([state[0][i][1], state[1][i][1]], axis=1) ## i-th layer, h state for LSTM\n",
    "            elif RNN_type == 'GRU':\n",
    "                tmp = tf.concat([state[0][i], state[1][i]], axis=1) ## i-th layer, h state for GRU\n",
    "            else:\n",
    "                print('ERROR: WRONG RNN CELL TYPE')\n",
    "        else:\n",
    "            if RNN_type == 'LSTM':\n",
    "                tmp = state[i][1] ## i-th layer, h state for LSTM\n",
    "            elif RNN_type == 'GRU':\n",
    "                tmp = state[i] ## i-th layer, h state for GRU\n",
    "            else:\n",
    "                print('ERROR: WRONG RNN CELL TYPE')\n",
    "\n",
    "        if i == 0:\n",
    "            rnn_state_out = tmp\n",
    "        else:\n",
    "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
    "    \n",
    "    return rnn_state_out\n",
    "\n",
    "\n",
    "### FEEDFORWARD NETWORK\n",
    "def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, w_reg=None, keep_prob=1.0):\n",
    "    '''\n",
    "        GOAL             : Create FC network with different specifications \n",
    "        inputs (tensor)  : input tensor\n",
    "        num_layers       : number of layers in FCNet\n",
    "        h_dim  (int)     : number of hidden units\n",
    "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
    "        o_dim  (int)     : number of output units\n",
    "        o_fn             : activation function for output layers (defalut: None)\n",
    "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
    "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
    "    '''\n",
    "    # default active functions (hidden: relu, out: None)\n",
    "    if h_fn is None:\n",
    "        h_fn = tf.nn.relu\n",
    "    if o_fn is None:\n",
    "        o_fn = None\n",
    "\n",
    "    # default initialization functions (weight: Xavier, bias: None)\n",
    "    if w_init is None:\n",
    "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        if num_layers == 1:\n",
    "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "        else:\n",
    "            if layer == 0:\n",
    "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "                if not keep_prob is None:\n",
    "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n",
    "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "                if not keep_prob is None:\n",
    "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "            else: # layer == num_layers-1 (the last layer)\n",
    "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq-KZXk5keOM"
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "import random\n",
    "\n",
    "##### USER-DEFINED FUNCTIONS\n",
    "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
    "    '''\n",
    "        mask1 is required to get the contional probability (to calculate the denominator part)\n",
    "        mask1 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
    "    '''\n",
    "    mask = np.zeros([np.shape(meas_time)[0], num_Event, num_Category]) # for denominator\n",
    "    for i in range(np.shape(meas_time)[0]):\n",
    "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
    "\n",
    "    return mask\n",
    "\n",
    "def f_get_minibatch(mb_size, x, x_mi, label, time, mask1, mask2, mask3):\n",
    "    idx = range(np.shape(x)[0])\n",
    "    idx = random.sample(idx, mb_size)\n",
    "\n",
    "    x_mb     = x[idx, :, :].astype(float)\n",
    "    x_mi_mb  = x_mi[idx, :, :].astype(float)\n",
    "    k_mb     = label[idx, :].astype(float) # censoring(0)/event(1,2,..) label\n",
    "    t_mb     = time[idx, :].astype(float)\n",
    "    m1_mb    = mask1[idx, :, :].astype(float) #fc_mask\n",
    "    m2_mb    = mask2[idx, :, :].astype(float) #fc_mask\n",
    "    m3_mb    = mask3[idx, :].astype(float) #fc_mask\n",
    "    return x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb\n",
    "\n",
    "\n",
    "def f_get_boosted_trainset(x, x_mi, time, label, mask1, mask2, mask3):\n",
    "    _, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(x)[0], 1]), np.cumsum(x[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    total_sample = 0\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        total_sample += np.sum(np.sum(x[i], axis=1) != 0)\n",
    "\n",
    "    new_label          = np.zeros([total_sample, np.shape(label)[1]])\n",
    "    new_time           = np.zeros([total_sample, np.shape(time)[1]])\n",
    "    new_x              = np.zeros([total_sample, np.shape(x)[1], np.shape(x)[2]])\n",
    "    new_x_mi           = np.zeros([total_sample, np.shape(x_mi)[1], np.shape(x_mi)[2]])\n",
    "    new_mask1          = np.zeros([total_sample, np.shape(mask1)[1], np.shape(mask1)[2]])\n",
    "    new_mask2          = np.zeros([total_sample, np.shape(mask2)[1], np.shape(mask2)[2]])\n",
    "    new_mask3          = np.zeros([total_sample, np.shape(mask3)[1]])\n",
    "\n",
    "    tmp_idx = 0\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        max_meas = np.sum(np.sum(x[i], axis=1) != 0)\n",
    "\n",
    "        for t in range(max_meas):\n",
    "            new_label[tmp_idx+t, 0] = label[i,0]\n",
    "            new_time[tmp_idx+t, 0]  = time[i,0]\n",
    "\n",
    "            new_x[tmp_idx+t,:(t+1), :] = x[i,:(t+1), :]\n",
    "            new_x_mi[tmp_idx+t,:(t+1), :] = x_mi[i,:(t+1), :]\n",
    "\n",
    "            new_mask1[tmp_idx+t, :, :] = f_get_fc_mask1(meas_time[i,t].reshape([-1,1]), num_Event, num_Category) #age at the measurement\n",
    "            new_mask2[tmp_idx+t, :, :] = mask2[i, :, :]\n",
    "            new_mask3[tmp_idx+t, :]    = mask3[i, :]\n",
    "\n",
    "        tmp_idx += max_meas\n",
    "        \n",
    "    return(new_x, new_x_mi, new_time, new_label, new_mask1, new_mask2, new_mask3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRJaXTVTkqDE"
   },
   "outputs": [],
   "source": [
    "# Logging Functions (Not Important)\n",
    "def save_logging(dictionary, log_name):\n",
    "    with open(log_name, 'w') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYDmJRwHlJGU",
    "outputId": "db865223-11dd-4f9a-e523-4a02cf769c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lifelines in /usr/local/lib/python3.7/dist-packages (0.26.3)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n",
      "Requirement already satisfied: formulaic<0.3,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.2.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
      "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.5.0)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
      "Requirement already satisfied: interface-meta>=1.2 in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.2.4)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.12.1)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l12_qvTbk3Jr"
   },
   "outputs": [],
   "source": [
    "# Evalutation Functions\n",
    "import numpy as np\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "\n",
    "### C(t)-INDEX CALCULATION\n",
    "def c_index(Prediction, Time_survival, Death, Time):\n",
    "    '''\n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky)\n",
    "        - Time_survival   : survival/censoring time\n",
    "        - Death           :\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
    "    '''\n",
    "    N = len(Prediction)\n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
    "  \n",
    "        if (Time_survival[i]<=Time and Death[i]==1):\n",
    "            N_t[i,:] = 1\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "### BRIER-SCORE\n",
    "def brier_score(Prediction, Time_survival, Death, Time):\n",
    "    N = len(Prediction)\n",
    "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
    "\n",
    "    return np.mean((Prediction - y_true)**2)\n",
    "\n",
    "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
    "\n",
    "\n",
    "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
    "def CensoringProb(Y, T):\n",
    "\n",
    "    T = T.reshape([-1]) # (N,) - np array\n",
    "    Y = Y.reshape([-1]) # (N,) - np array\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
    "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
    "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "### C(t)-INDEX CALCULATION\n",
    "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    '''\n",
    "        This is a cause-specific c(t)-index\n",
    "        - Prediction      : risk at Time (higher --> more risky)\n",
    "        - Time_survival   : survival/censoring time\n",
    "        - Death           :\n",
    "            > 1: death\n",
    "            > 0: censored (including death from other cause)\n",
    "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
    "    '''\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "\n",
    "    N = len(Prediction)\n",
    "    A = np.zeros((N,N))\n",
    "    Q = np.zeros((N,N))\n",
    "    N_t = np.zeros((N,N))\n",
    "    Num = 0\n",
    "    Den = 0\n",
    "    for i in range(N):\n",
    "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
    "\n",
    "        if len(tmp_idx) == 0:\n",
    "            W = (1./G[1, -1])**2\n",
    "        else:\n",
    "            W = (1./G[1, tmp_idx[0]])**2\n",
    "\n",
    "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
    "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
    "\n",
    "        if (T_test[i]<=Time and Y_test[i]==1):\n",
    "            N_t[i,:] = 1.\n",
    "\n",
    "    Num  = np.sum(((A)*N_t)*Q)\n",
    "    Den  = np.sum((A)*N_t)\n",
    "\n",
    "    if Num == 0 and Den == 0:\n",
    "        result = -1 # not able to compute c-index!\n",
    "    else:\n",
    "        result = float(Num/Den)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
    "    G = CensoringProb(Y_train, T_train)\n",
    "    N = len(Prediction)\n",
    "\n",
    "    W = np.zeros(len(Y_test))\n",
    "    Y_tilde = (T_test > Time).astype(float)\n",
    "\n",
    "    for i in range(N):\n",
    "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
    "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
    "\n",
    "        if len(tmp_idx1) == 0:\n",
    "            G1 = G[1, -1]\n",
    "        else:\n",
    "            G1 = G[1, tmp_idx1[0]]\n",
    "\n",
    "        if len(tmp_idx2) == 0:\n",
    "            G2 = G[1, -1]\n",
    "        else:\n",
    "            G2 = G[1, tmp_idx2[0]]\n",
    "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
    "\n",
    "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
    "\n",
    "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmwK1mlHmgHX"
   },
   "outputs": [],
   "source": [
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    '''\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    '''\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sK-sTisytAf"
   },
   "source": [
    "$F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REcrZt_Wqlqt"
   },
   "outputs": [],
   "source": [
    "data_mode                   = 'PBC2' \n",
    "seed                        = 1234\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = max event/censoring time * 1.2\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (1 + num_features)\n",
    "    x_dim_cont              = dim of continuous features\n",
    "    x_dim_bin               = dim of binary features\n",
    "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
    "'''\n",
    "\n",
    "if data_mode == 'PBC2':\n",
    "    (x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi) = import_dataset(norm_mode = 'standard')\n",
    "    \n",
    "    # This must be changed depending on the datasets, prediction/evaliation times of interest\n",
    "    pred_time = [52, 3*52, 5*52] # prediction time (in months)\n",
    "    eval_time = [12, 36, 60, 120] # months evaluation time (for C-index and Brier-Score)\n",
    "else:\n",
    "    print ('ERROR:  DATA_MODE NOT FOUND !!!')\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "max_length                  = np.shape(data)[1]\n",
    "\n",
    "\n",
    "file_path = '{}'.format(data_mode)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztBC4EP3qww3"
   },
   "outputs": [],
   "source": [
    "urn_in_mode                = 'ON' #{'ON', 'OFF'}\n",
    "boost_mode                  = 'ON' #{'ON', 'OFF'}\n",
    "\n",
    "##### HYPER-PARAMETERS\n",
    "new_parser = {'mb_size': 32,\n",
    "\n",
    "             'iteration_burn_in': 3000,\n",
    "             'iteration': 25000,\n",
    "\n",
    "             'keep_prob': 0.6,\n",
    "             'lr_train': 1e-4,\n",
    "\n",
    "             'h_dim_RNN': 100,\n",
    "             'h_dim_FC' : 100,\n",
    "             'num_layers_RNN':2,\n",
    "             'num_layers_ATT':2,\n",
    "             'num_layers_CS' :2,\n",
    "\n",
    "             'RNN_type':'LSTM', #{'LSTM', 'GRU'}\n",
    "\n",
    "             'FC_active_fn' : tf.nn.relu,\n",
    "             'RNN_active_fn': tf.nn.tanh,\n",
    "\n",
    "            'reg_W'         : 1e-5,\n",
    "            'reg_W_out'     : 0.,\n",
    "\n",
    "             'alpha' :1.0,\n",
    "             'beta'  :0.1,\n",
    "             'gamma' :1.0\n",
    "}\n",
    "\n",
    "\n",
    "# INPUT DIMENSIONS\n",
    "input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                'x_dim_cont'    : x_dim_cont,\n",
    "                                'x_dim_bin'     : x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "# NETWORK HYPER-PARMETERS\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : new_parser['FC_active_fn'],\n",
    "                                'RNN_active_fn'     : new_parser['RNN_active_fn'],\n",
    "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : new_parser['reg_W_out']\n",
    "                                 }\n",
    "\n",
    "\n",
    "mb_size           = new_parser['mb_size']\n",
    "iteration         = new_parser['iteration']\n",
    "iteration_burn_in = new_parser['iteration_burn_in']\n",
    "\n",
    "keep_prob         = new_parser['keep_prob']\n",
    "lr_train          = new_parser['lr_train']\n",
    "\n",
    "alpha             = new_parser['alpha']\n",
    "beta              = new_parser['beta']\n",
    "gamma             = new_parser['gamma']\n",
    "\n",
    "# SAVE HYPERPARAMETERS\n",
    "log_name = file_path + '/hyperparameters_log.txt'\n",
    "save_logging(new_parser, log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMmXZuo5rFoB"
   },
   "outputs": [],
   "source": [
    "### TRAINING-TESTING SPLIT\n",
    "(tr_data,te_data, tr_data_mi, te_data_mi, tr_time,te_time, tr_label,te_label, \n",
    " tr_mask1,te_mask1, tr_mask2,te_mask2, tr_mask3,te_mask3) = train_test_split(data, data_mi, time, label, mask1, mask2, mask3, test_size=0.2, random_state=seed) \n",
    "\n",
    "(tr_data,va_data, tr_data_mi, va_data_mi, tr_time,va_time, tr_label,va_label, \n",
    " tr_mask1,va_mask1, tr_mask2,va_mask2, tr_mask3,va_mask3) = train_test_split(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3, test_size=0.2, random_state=seed) \n",
    "\n",
    "if boost_mode == 'ON':\n",
    "    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz0fEUWdrK6O",
    "outputId": "6df2a42e-a583-49ee-970a-9b84a5bc03ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "MAIN TRAINING ...\n",
      "itr: 1000 | loss: 239.2019\n",
      "updated.... average c-index = 0.6072\n",
      "itr: 2000 | loss: 250.2392\n",
      "updated.... average c-index = 0.6125\n",
      "itr: 3000 | loss: 189.3664\n",
      "itr: 4000 | loss: 174.0380\n",
      "itr: 5000 | loss: 160.4915\n",
      "itr: 6000 | loss: 250.2144\n",
      "itr: 7000 | loss: 88.0602\n",
      "itr: 8000 | loss: 39.4516\n",
      "itr: 9000 | loss: 91.3234\n",
      "itr: 10000 | loss: 69.7258\n",
      "itr: 11000 | loss: 111.0799\n",
      "itr: 12000 | loss: 32.8524\n",
      "itr: 13000 | loss: 74.1220\n",
      "itr: 14000 | loss: 37.0484\n",
      "itr: 15000 | loss: 134.1635\n",
      "itr: 16000 | loss: 61.5247\n",
      "itr: 17000 | loss: 96.1103\n",
      "itr: 18000 | loss: 53.9188\n",
      "itr: 19000 | loss: 114.7293\n",
      "itr: 20000 | loss: 162.0449\n",
      "itr: 21000 | loss: 70.0011\n",
      "itr: 22000 | loss: 92.2969\n",
      "itr: 23000 | loss: 36.3737\n",
      "itr: 24000 | loss: 47.9565\n",
      "itr: 25000 | loss: 56.1713\n"
     ]
    }
   ],
   "source": [
    "##### CREATE DYNAMIC-DEEPFHT NETWORK\n",
    "tf.reset_default_graph()\n",
    "\n",
    "burn_in_mode = \"OFF\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    " \n",
    "### TRAINING - BURN-IN\n",
    "if burn_in_mode == 'ON':\n",
    "    print( \"BURN-IN TRAINING ...\")\n",
    "    for itr in range(iteration_burn_in):\n",
    "        x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
    "        DATA = (x_mb, k_mb, t_mb)\n",
    "        MISSING = (x_mi_mb)\n",
    "\n",
    "        _, loss_curr = model.train_burn_in(DATA, MISSING, keep_prob, lr_train)\n",
    "\n",
    "        if (itr+1)%1000 == 0:\n",
    "            print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
    "\n",
    "\n",
    "### TRAINING - MAIN\n",
    "print( \"MAIN TRAINING ...\")\n",
    "min_valid = 0.5\n",
    "\n",
    "for itr in range(iteration):\n",
    "    x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
    "    DATA = (x_mb, k_mb, t_mb)\n",
    "    MASK = (m1_mb, m2_mb, m3_mb)\n",
    "    MISSING = (x_mi_mb)\n",
    "    PARAMETERS = (alpha, beta, gamma)\n",
    "\n",
    "    _, loss_curr = model.train(DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train)\n",
    "\n",
    "    if (itr+1)%1000 == 0:\n",
    "        print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
    "\n",
    "    ### VALIDATION  (based on average C-index of our interest)\n",
    "    if (itr+1)%1000 == 0:        \n",
    "        risk_all = f_get_risk_predictions(sess, model, va_data, va_data_mi, pred_time, eval_time)\n",
    "        \n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            pred_horizon = int(p_time)\n",
    "            val_result1 = np.zeros([num_Event, len(eval_time)])\n",
    "            \n",
    "            for t, t_time in enumerate(eval_time):                \n",
    "                eval_horizon = int(t_time) + pred_horizon\n",
    "                for k in range(num_Event):\n",
    "                    val_result1[k, t] = c_index(risk_all[k][:, p, t], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            \n",
    "            if p == 0:\n",
    "                val_final1 = val_result1\n",
    "            else:\n",
    "                val_final1 = np.append(val_final1, val_result1, axis=0)\n",
    "\n",
    "        tmp_valid = np.mean(val_final1)\n",
    "\n",
    "        if tmp_valid >  min_valid:\n",
    "            min_valid = tmp_valid\n",
    "            saver.save(sess, file_path + '/model')\n",
    "            print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbiCw72FrO32",
    "outputId": "8089b24d-7165-4a91-dbac-1be80e97dc47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from PBC2/model\n",
      "========================================================\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "                        eval_time 12  eval_time 36  eval_time 60  eval_time 120\n",
      "pred_time 52: event_2       0.966942      0.890000      0.866667       0.915746\n",
      "pred_time 52: event_2      -1.000000     -1.000000     -1.000000       0.807692\n",
      "pred_time 156: event_2      0.936102      0.901554      0.886443       0.911890\n",
      "pred_time 156: event_2      0.942308      0.939394      0.881119       0.887097\n",
      "pred_time 260: event_2      0.888535      0.899151      0.899898       0.895522\n",
      "pred_time 260: event_2      0.887097      0.768817      0.790323       0.800797\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "                        eval_time 12  eval_time 36  eval_time 60  eval_time 120\n",
      "pred_time 52: event_2       0.027765      0.033911      0.034706       0.087650\n",
      "pred_time 52: event_2       0.006032      0.007514      0.007566       0.021156\n",
      "pred_time 156: event_2      0.080935      0.102832      0.114614       0.130933\n",
      "pred_time 156: event_2      0.018135      0.024168      0.028031       0.034507\n",
      "pred_time 260: event_2      0.153711      0.153971      0.161822       0.178470\n",
      "pred_time 260: event_2      0.035854      0.036075      0.039545       0.060332\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, file_path + '/model')\n",
    "\n",
    "risk_all = f_get_risk_predictions(sess, model, te_data, te_data_mi, pred_time, eval_time)\n",
    "\n",
    "for p, p_time in enumerate(pred_time):\n",
    "    pred_horizon = int(p_time)\n",
    "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "    for t, t_time in enumerate(eval_time):                \n",
    "        eval_horizon = int(t_time) + pred_horizon\n",
    "        for k in range(num_Event):\n",
    "            result1[k, t] = c_index(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            result2[k, t] = brier_score(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    \n",
    "    if p == 0:\n",
    "        final1, final2 = result1, result2\n",
    "    else:\n",
    "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "        \n",
    "        \n",
    "row_header = []\n",
    "for p_time in pred_time:\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "            \n",
    "col_header = []\n",
    "for t_time in eval_time:\n",
    "    col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "# c-index result\n",
    "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# brier-score result\n",
    "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "### PRINT RESULTS\n",
    "print('========================================================')\n",
    "print('--------------------------------------------------------')\n",
    "print('- C-INDEX: ')\n",
    "print(df1)\n",
    "print('--------------------------------------------------------')\n",
    "print('- BRIER-SCORE: ')\n",
    "print(df2)\n",
    "print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zCObDC_xRRT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepHIT Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
